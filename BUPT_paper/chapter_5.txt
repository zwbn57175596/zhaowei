第五章 分布式系统的设计


服务模块化的设计
    对服务进行模块划分，有助于对业务进行更清晰的梳理。同时在软件设计的层面上，好的模块划分更有利于程序员架构师对项目的维护与扩展，减少开始的代码量。
    模块划分是分布式服务设计的第一步。有了上面的业务需求，对工作圈的模块第一步的划分很好理解，根据上面的需求功划分能得到：用户、圈子（贴子）、评论、赞、IM消息、工作功能等模块服务。因为贴子也是圈子的一个成员属性故划到圈子服务中。而评论和赞的服务，可以通用化用在其它的一些业务上，比如用在工作的审批中。所以做为一个基础的服务可以独立出来。

    所有的业务模块，基本都需要操作数据库与缓存。所以将对MongoDB数据库与Redis缓存的连接与操作服务封装成公共模块的服务，开发时要求开发人员统计使用这些公共服务完成对持久层的开发。这样不仅可以减少在持久层上的的代码开发量，同时也可以应用链接池技术提升服务的吞吐性能，预防因为人为的疏忽导致的链接泄漏。

    在进行模块划分设计需要考虑的要点：
    1.高内聚，低耦合。模块之前要相互独立，仅数据耦合。
    2.提取功能相同的代码到一个公共服务包进封装。减少开发代码量。
    3.模块服务要是无状态的，便于模块服务的模向扩容，同时也为restful服务打基础

    最终模块划分如下：
    公共模块：
    gongzuoquan-mongo，提供数据库接入操作的封装。
    gongzuoquan-cache，提供对缓存服务的接入操作。
    gongzuoquan-configcenter，提供自动化配置中的管理操作。
    gongzuoquan-util，提供一些公共的工具类方法

    核心业务服务
    gongzuoquan-idcenter，提供全局主键生成服务。
    gongzuoquan-idlist，提供数据主键索引中心。
    gongzuoquan-setting，提供用户个性化设置配置服务。

    业务服务
    gongzuoquan-account，用户账户服务
    gongzuoquan-quan，圈子、贴子服务
    gongzuoquan-comment，评论服务
    gongzuoquan-favorite，赞服务
    gongzuoquan-app，工作应用服务



分布式服务与自动化配置中心
    模块划分完成之后，这个框架还不能被称为分布式框架。因为它还没有满足前面（3.3 现代分布式系统的特点）中提到的特性。下面来尝试进一步的满足这些特性。

    横向可扩展性（Scale Out）。意味着一个业务服务模块是可以部署很多个副本，这样就带来一个问题，如何保证所有的请求都能平均的分发到所有的服务上去？这时候我们需要一个自动化配置中心。在一个模块服务启动时，将启动的服务器在网络上的IP地址和提供服务的端口注册到自动化配置中心服务中，这样就完成了服务状态的发布。然后任一客户端在需要调用这个服务的时候，先在自动化配置中心中读取一下这个服务的服务状态，取得了可以提供这个服务的服务器地址列表，然后再以一个算法随机的地址发起请求，完成一次调用。这样就满足了服务的横向可扩展性（Scale Out）。

    不允许单点失效（No Single Point Failure）。只要对自动化配置中心的功能稍加改动，便能满足这一特性。只要能将服务状态列表中的故障节点检测到，并从中剔出除们，客户端便不会受到影响。只要这个服务状态列表中仍有一台服务器上的服务是正常的，就会不造成整个服务失效。当然，这里我们也要考虑到这个自动化配置中心服务本身会出问题，需要保证这组服务也是高可用的服务。

    对故障列表的检测，有两种方式：1.在启动的服务与自动化配置中心之间建立一个心跳，心跳消失即失效；2.从客户端来检测，如果客户端调用服务失败后，将其从自动化配置中心中剔除然后访问下一个服务地址。第一种方式，因为心跳会有延时。如果在延时中发生问题，那么在这一段时间内就会有一定机率的请求会因为请求到故障节点上。第二种方式，需要做到客户端实时监听自动化配置中心的服务器状态变化，这样虽然会避免请求故障节点，但是在客户端数量过大的时候会大量的占用自动化配置中心服务的链接。工作圈目前的设计，客户端服务还是可控的，所以选择了第二种实时性较高的处理方式。

    在实际的开发中，Apache Zookeeper服务成为了自动化配置中心服务的不二之选。工作圈在Zookeeper约定以/gongzuoquan为根目录，在下面以模块名建立服务配置与服务状态的目录。客户端只要建立相应节点上的监听就可以得到相应服务的当前服务状态。同样，也可以将数据库和缓存服务的地址与链接配置存到Zookeeper中相应的节点下。当服务器的数据库或是缓存进行迁移数所据的时候，监听节点的变化，重新建立指向新服务的链接池就可以了。


数据分片(Data Sharding)与数据索引中心
    大数据时代，数据的持久化性能会影响服务的整体的性能。如果一个模块下的业务都存储在一个表中，那么随着数据量的上升性能必然会下降。解决的办法是对数据库中的数据进行水平扩展(分表，分库等)，控制住单表中的数据量不会超过一个阀值即可。这个阀值的界定不是固定的，会根据数据库的选择以及运行的环境不同而不同。比如我们需要数据库插入一条业务数据的响应时间要在1ms以内，需要我们在实际的运行环境中对数据库进行一个insert压力测试，得到在多少条数据的情况下单表的插入响应时间超过了1ms（这个时间需要根据业务场景来设定），那么这个阀值就是我们估算的一个单表数据量上限的指标。假设这个值是f，需求预估的整个系统容量是m，那个分表数t = m / f。如果数据库服务器不只一台，那么我们可以再将数据散列到不同的数据库上去，此时假设有n台数据库服务，那么t = m / (f * n)。

    对数据进行分表，分库的插入的过程我们叫数据分片((Data Sharding)。当系统分片的策略有很多，例如常见的有以下几种：
    根据ID特征：例如对记录的ID取模，得到的结果是几，那么这条记录就放在编号为几的数据分区上。
    根据时间范围：例如前100万个用户数据在第1个分区中，第二个100万用户数据放在第2个分区中。
    基于检索表：根据ID先去一个表内找到它所在的分区，然后再去目标分区进行查找。

    在这些数据分片策略之中没有哪个有绝对的优势，选择哪种策略完全是根据系统的业务或是数据特征来确定的。值得强调的是：数据分片不是银弹，它对系统的性能和伸缩性（Scalability）带来一定好处的同时，也会对系统开发带来许多复杂度。例如，有两条记录分别处在不同的服务器上，那么如果有一个业务是为它们建立一个“关联”，那么很可能表示“关联”的记录就必须在两个分区内各放一条。另外，如果您重视数据的完整性，那么跨数据分区的事务又立即变成了性能杀手。最后，如果有一些需要进行全局查找的业务，光有数据分片策略也很难对系统性能带来什么优势。

    工作圈的数据分片主要使用的是ID取模的方式，但是对这个策略进行了改进。首先引入一个gongzuoquan-idcenter的服务，用来随机生成数据在工作圈服务内全局唯一的主键id。这样，id在进行取模分片的时候就可以较为均匀的分布在所有的分表中。

    同时，为了解决跨区数据查询的问题，我们需要一个数据索引中心（类拟于hadoop的nameService的概念)。相一组有关系的数据id索引在一起保存起来，在查询的时候通过这个索引可以得到这些数据的id，然后再通过id取模来得到分库分表的数据，从而查询到需要的业务数据。工作圈中承担这部份工作的核心服务是gongzuoquan-idlist。


Rest server设计
    Rest server是整个工作圈服务的表现层，是和工作圈移动端或是WEB端通信的一层。工作圈终端借由Rest Server这一层提供的Rest api，来和工作圈服务进行数据的交换。

    分布式的Rest server。
    1.Nginx负载均衡
    分布式的Rest server意味着一个Rest服务器可以运行在多个服务器上。那么就需要一个前端的负载均衡来分发从互联网传来的请求。可用的服务也很多，Nginx是一个开源的优秀的http服务器，可以满足目前的需求。

    2.无状态的服务设计
    其实提出这点反而是为了解决用户的登录状态问题，假设用户登录的请求发到了Rest A服务上，对用户的数据请求又发送到了Rest B服务上。那Rest B怎么能知道用户的登录状态呢。在以前的单点式系统中，有一个Session的概念。用户登录后建立在服务器，请求的时候服务端会查询这个Session，登出后销毁。但是在分布式的Rest server上主机之前内存的数据不共享，无法得知用户的Session状态。
    工作圈的解决方式是登录后返回一个加密的token给终端。不是在服务端保存用户的登录状态，而是在用户使用的终端上保存这个token。在Android iOS系统上是由工作圈App软件保存在手机的数据库中，PC WEB因为没有数据库，将这个token保存在Cookie中。在每次请求的时候将WEB端会自动的带上Cookie供服务器查询，手机端的App需要程序从手机上读取这个请求加到Http Header中来便于服务检查。

